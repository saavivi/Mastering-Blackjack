from agents.BaseAgent import BaseAgent
import numpy as np
from collections import defaultdict
from lib.constants import TRAINING_DURATION, NUM_SHOW
import sys


def get_epsilon_greedy_probs(Q_s, epsilon, nA)->np.ndarray:
    """
    return an array of probabilities using the epsilon greedy approach
    with probability 1-epsilon choose the best action according to its q value
    with probability epsilon choose a random action
    :param Q_s: Q values array for all actions
    :param epsilon: float64
    :param nA: action space size
    :return: array of action probabilities
    """
    policy_s = np.ones(nA) * epsilon / nA
    best_a = np.argmax(Q_s)
    policy_s[best_a] = 1 - epsilon + (epsilon / nA)
    return policy_s


def generate_episode_from_q(env, Q, epsilon, nA):
    """
    Generate an episode while using the Q value function and epsilon-greedy
    policy
    :param env: gym env
    :param Q: Qsa value function
    :param epsilon: the epsilon value for the epsilon greedy policy
    :param nA: action space size
    :return: a list of (state,action,reward) representing the episode generated
    :rtype: list
    """
    episode = []
    state = env.reset()
    while True:
        action = np.random.choice(np.arange(nA),
                                  p=get_epsilon_greedy_probs(Q[state], epsilon, nA)) \
            if state in Q else env.action_space.sample()
        next_state, reward, done, info = env.step(action)
        episode.append((state, action, reward))
        state = next_state
        if done:
            break
    return episode


def update_q(episode, Q, alpha, gamma):
    """
    Update Q in respective to the Bellman equation:
    Q(s,a) = Q(s,a) + alpha*(G_t - Q(s,a))
    where G_t = R1 + gamma*R2 + gamma^2*R3 ... or sum(gamma^t*Reward_t)
    :param episode: A list representing an episode, generated by
                    generate_episode_from_Q
    :param Q: Q value function
    :param alpha: decay factor
    :param gamma: discount factor
    :return: the updated Q value function
    """
    states, actions, rewards = zip(*episode)
    discounts = np.array([gamma ** i for i in range(len(rewards) + 1)])
    for i, state in enumerate(states):
        old_q = Q[state][actions[i]]
        Q[state][actions[i]] = old_q + alpha * (
                    sum(rewards[i:] * discounts[:-(1 + i)]) - old_q)
    return Q


def mc_control(env, num_episodes=TRAINING_DURATION, alpha=1.0, gamma=1.0, eps_start=1.0, eps_decay=.99999, eps_min=0.05):
    """
    Use the Monte Carlo control algorithm to in order to generate a Q value
    function and the best policy.
    MC algorithm in a nutshell:
    For each state in episode update Q(s,a)
    :param env: The gym environment
    :param num_episodes: The number of episodes to use for training
    :param alpha: decay factor
    :param gamma: discount factor
    :param eps_start: epsilon starting value
    :param eps_decay: epsilon decay factor
    :param eps_min: minimum epsilon
    :return: Q(s,a) value function and the optimal policy
    """
    nA = env.action_space.n
    Q = defaultdict(lambda: np.zeros(nA))
    epsilon = eps_start
    for i_episode in range(1, num_episodes+1):
        if i_episode % (num_episodes//10) == 0:
            print("\rEpisode {}/{}.".format(i_episode, num_episodes), end="")
            sys.stdout.flush()
        epsilon = max(epsilon*eps_decay, eps_min)
        episode = generate_episode_from_q(env, Q, epsilon, nA)
        Q = update_q(episode, Q, alpha, gamma)
    policy = dict((k, np.argmax(v)) for k, v in Q.items())
    return policy, Q


class MonteCarloAgent(BaseAgent):

    def __init__(self):
        super().__init__()

    def train(self):
        self.policy, self.q = mc_control(self._env, num_episodes=TRAINING_DURATION, alpha=0.015)


if __name__ == "__main__":
    for i in range(5):
        mc_agent = MonteCarloAgent()
        mc_agent.train()
        mc_agent.plot_policy()
        mc_agent.play()
