from agents.BaseAgent import BaseAgent
import numpy as np
from collections import defaultdict
from lib.constants import TRAINING_DURATION, EVALUATE_NUM_OF_HANDS,EVALUATE_EVERY
from lib.utils import tournament


def get_epsilon_greedy_probs(Q_s, epsilon, nA)->np.ndarray:
    """
    return an array of probabilities using the epsilon greedy approach
    with probability 1-epsilon choose the best action according to its q value
    with probability epsilon choose a random action
    :param Q_s: Q values array for all actions
    :param epsilon: float64
    :param nA: action space size
    :return: array of action probabilities
    """
    policy_s = np.ones(nA) * epsilon / nA
    best_a = np.argmax(Q_s)
    policy_s[best_a] = 1 - epsilon + (epsilon / nA)
    return policy_s


def generate_episode_from_q(env, Q, epsilon, nA):
    """
    Generate an episode while using the Q value function and epsilon-greedy
    policy
    :param env: gym env
    :param Q: Qsa value function
    :param epsilon: the epsilon value for the epsilon greedy policy
    :param nA: action space size
    :return: a list of (state,action,reward) representing the episode generated
    :rtype: list
    """
    episode = []
    state = env.reset()
    while True:
        action = np.random.choice(np.arange(nA),
                                  p=get_epsilon_greedy_probs(Q[state], epsilon, nA)) \
            if state in Q else env.action_space.sample()
        next_state, reward, done, info = env.step(action)
        episode.append((state, action, reward))
        state = next_state
        if done:
            break
    return episode


def update_q(episode, Q, alpha, gamma):
    """
    Update Q in respective to the Bellman equation:
    Q(s,a) = Q(s,a) + alpha*(G_t - Q(s,a))
    where G_t = R1 + gamma*R2 + gamma^2*R3 ... or sum(gamma^t*Reward_t)
    :param episode: A list representing an episode, generated by
                    generate_episode_from_Q
    :param Q: Q value function
    :param alpha: decay factor
    :param gamma: discount factor
    :return: the updated Q value function
    """
    states, actions, rewards = zip(*episode)
    discounts = np.array([gamma ** i for i in range(len(rewards) + 1)])
    for i, state in enumerate(states):
        old_q = Q[state][actions[i]]
        Q[state][actions[i]] = old_q + alpha * (
                    sum(rewards[i:] * discounts[:-(1 + i)]) - old_q)
    return Q


def mc_control(env, to_train, already_trained=0, q=None, alpha=1.0, gamma=1.0,
               eps_start=1.0, eps_decay=.99999, eps_min=0.05):
    """
    Use the Monte Carlo control algorithm to in order to generate a Q value
    function and the best policy.
    MC algorithm in a nutshell:
    For each state in episode update Q(s,a)
    :param env: The gym environment
    :param num_episodes: The number of episodes to use for training
    :param alpha: decay factor
    :param gamma: discount factor
    :param eps_start: epsilon starting value
    :param eps_decay: epsilon decay factor
    :param eps_min: minimum epsilon
    :param logger:
    :return: Q(s,a) value function and the optimal policy
    """
    nA = env.action_space.n
    if q is None:
        q = defaultdict(lambda: np.zeros(nA))
    epsilon = eps_start*(eps_decay**already_trained)
    for i_episode in range(1, to_train + 1):
        # if i_episode % (num_episodes // 10) == 0:
        #     print("\rTraining: Episode {}/{}.".format(i_episode,
        #                                               num_episodes),
        #           end="")
        #     sys.stdout.flush()
        epsilon = max(epsilon * eps_decay, eps_min)
        episode = generate_episode_from_q(env, q, epsilon, nA)
        q = update_q(episode, q, alpha, gamma)
    policy = dict((k, np.argmax(v)) for k, v in q.items())
    return policy, q


class MonteCarloAgent(BaseAgent):

    def __init__(self):
        super().__init__(log_dir='./experiments/mc_result/')

    def train(self):
        for i in range(0, TRAINING_DURATION // EVALUATE_EVERY + 1):
            self.logger.log_performance(i * EVALUATE_EVERY,
                                        tournament(self._env,
                                                   EVALUATE_NUM_OF_HANDS)[0])
            # Best Alpha so far is 0.015
            self.eval_policy, self.q = mc_control(self._env,
                                                  q=self.q,
                                                  to_train=EVALUATE_EVERY,
                                                  already_trained=EVALUATE_EVERY*i,
                                                  alpha=0.015,
                                                  gamma=1.0,
                                                  eps_start=1.0,
                                                  eps_decay=0.99999,
                                                  eps_min=0.015
                                                  )
        self.policy = self.eval_policy


if __name__ == "__main__":
    mc_agent = MonteCarloAgent()
    mc_agent.train()
    mc_agent.plot_policy()
    mc_agent.plot('MC')
